## BERT QA

* BERT模型采用双向Transformer Encoder产生的编码如何融合？

* BERT参数量
设Transformer层数为L,隐层大小为H,自注意力头数为A。
\>> $BERT_{BASE}$:L=12,H=768,A=12,参数量为110M,即1.1亿

    $BERT_{LARGE}$:L=24,H=1024,A=16,参数数量为340M，即3.4亿