## BERT QA

* BERT模型采用双向Transformer Encoder产生的编码如何融合？

* [BERT参数量](https://blog.csdn.net/weixin_43922901/article/details/102602557)

设Transformer层数为L,隐层大小为H,自注意力头数为A。
\>> $BERT_{BASE}$:L=12,H=768,A=12,参数量为110M,即1.1亿

    $BERT_{LARGE}$:L=24,H=1024,A=16,参数数量为340M，即3.4亿