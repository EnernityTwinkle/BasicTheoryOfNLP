### Sigmoid激活函数

$$\sigma(x)=\frac{1}{1+e^{-x}}$$
缺点：

会有梯度弥散现象；

不是关于原点对称；

计算exp比较耗时。

### Tanh激活函数

$$tanh(x)=2\sigma(2x)-1$$
缺点：

梯度弥散没有解决；

优点：

解决了原点对称问题；

比sigmoid更快；

### ReLU函数

$$f(x)=max(0,x)$$
缺点：

梯度弥散没有完全解决，在（-）部分相当于神经元死亡，不能被更新；

优点：

解决了部分梯度弥散的问题；

收敛速度更快。

### Leaky ReLU

$$f(x)=max(\varepsilon x, x)$$
其中$\varepsilon$是一个很小的负梯度值，比如0.01。这样做的目的是使负轴信息不会全部丢失，解决了ReLU神经元“死掉”的问题。

### Maxout

### GELUs[<sup>1</sup>](#GELUs-csdn-1)（高斯）
$$GELU(x)=xP(X<=x>)=x \Phi(x)$$

### Softmax
$$\sigma(z)_j=\frac{e^{z_j}}{\sum_{k=1}^{K}e^{z_k}}$$

## 参考文献
<div id='GELUs-1'></div>
- [1] [GELUs-csdn-1] (https://blog.csdn.net/liruihongbob/article/details/86510622)