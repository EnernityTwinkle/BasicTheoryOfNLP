## 发展历程

神经网络语言模型（NNLM，2003）->Word2Vec（2013）->Glove->ELMO（2018）->GPT->BERT->RoBERTa

### ELMO
编码器：双向LSTM

### GPT
编码器：Transformer

训练任务：单向语言模型

## RoBERTa（一个强基准）
在BERT的基础上进行更充分的训练（使用更多数据，放大BatchSize，增加预训练步数），同时去掉了预训练任务中的Next Sentence Predict。

## ALBERT[<sup>1</sup>](#ALBERT-1)

提出了Sentence-order prediction (SOP)来取代NSP。具体来说，其正例与NSP相同，但负例是通过选择一篇文档中的两个连续的句子并将它们的顺序交换构造的。这样两个句子就会有相同的话题，模型学习到的就更多是句子间的连贯性。

## XLNET


## 提高预训练语言模型性能的常见手段
1、更高质量、更多数量的预训练数据

2、增加模型的容量及复杂度

3、更充分地训练模型：一般指放大BatchSize,增加预训练步数

4、有难度的预训练任务

## 参考文献
<div id="ALBERT-1"></div>
- [1] [ALBERT-知乎](https://zhuanlan.zhihu.com/p/87562926)