# Stacking方法

训练一个模型用于组合其他各个模型，即首先训练多个不同的模型，然后再以之前训练的各个模型的输出作为输入来训练一个模型，以得到最终的输出。

# Boosting

# Bagging
# 常见问题

### GBDT和XGBOOST的区别

* 传统GBDT以CART决策树作为基分类器，XGBOOST还支持线性分类器
* 传统GBDT在优化时只用到一阶导数信息，XGBOOST则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。