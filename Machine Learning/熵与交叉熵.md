## 熵

### 定义

熵在信息论中代表随机变量不确定度的度量。一个离散型随机变量$X$的熵$H(X)$定义为：
$$H(X)=-\sum_{x\in X}p(x)logp(x)$$

信息熵的三条性质：

* 单调性：发生概率越高的事件，其所携带的信息熵越低。
* 非负性：信息熵不能为负
* 累加性：多随机事件同时发生存在的总不确定性的度量可以表示为各事件不确定性的量度的和。

信息熵本质上是对我们司空见惯的“不确定现象”的数学化度量。比如说，如果天气预报说“今天中午下雨的可能性是百分之九十”，我们就会不约而同想到出门带伞；如果预报说“有百分之五十的可能性下雨”，我们就会犹豫是否带伞，因为雨伞无用时确是累赘之物。显然，第一则天气预报中，下雨这件事的不确定性程度较小（熵的值小），而第二则关于下雨的不确定度就大多了（熵的值大）。

## KL散度
用于衡量两个分布之间的差异。

### 定义

对于离散事件，我们可以定义事件A和B的差别为：
$$D_{KL}(A||B)=\sum_i{P_A(x_i)log(\frac{P_A(x_i)}{P_B(x_i)})}=\sum_i{P_A(x_i)log(P_A(x_i))-P_A(x_i)log(P_B(x_i))}$$

## 交叉熵

### 定义

$$H(A,B)=-\sum_i{P_A(x_i)log(P_B(x_i))}$$

如果A的熵$S(A)$是一个常量，那么$D_{KL}(A||B)=H(A,B)$，即KL散度和交叉熵在特定条件下等价。