## 交叉熵损失函数（适合衡量两个概率分布的差异）

$$H(y^{(i)}, \hat{y}^{(i)}) = -\sum_{j=1}^{n}y_j^{(i)}log\hat{y}_j^{(i)}$$

其中 $y^{(i)}$ 是真实的概率分布， $\hat{y}^{(i)}$ 是模型预测的概率分布。

## Hinge损失函数(SVM使用的损失函数)

Hinge损失函数标准形式如下：

$L(y,f(x))=max(0,1-yf(x))$,

其中$y$是真实label(-1或1)，$f(x)$是预测值，在-1到1之间。

特点：

（1）hinge损失函数表示如果分类正确，损失为0，否则损失为$1-yf(x)$

（2）健壮性相对较高，对异常点、噪声不敏感，但没有太好的概率解释。

## 平方损失函数

平方损失函数标准形式如下：

$L(Y|f(X))=\sum_{N}(Y-f(X))^2$

特点：经常应用与回归问题中。

## 0-1损失函数

0-1损失是指预测值和目标值不相等为1，否则为0：
$$
L(Y,f(X))= \left \{
\begin{matrix}
1,Y\neq f(X)\\ 
0,Y=f(X) \\
\end{matrix} 
\right. 
$$

特点：

（1）0-1损失函数直接对应分类错误的个数

（2）感知机使用的就是这种损失函数。但相等这个条件过于严格，因此可以放宽条件，即满足$|Y-f(x)|<T$时认为相等。

# 相关知识问答

### 交叉熵函数与最大似然函数的联系和区别[<sup>1</sup>](#loss-function-1)？

\>>区别：交叉熵函数用来描述模型预测值和真实值的差距大小，越大代表越不相近；似然函数的本质是衡量在某个参数下，整体的估计和真实情况一样的概率，越大表示越相近。

联系：交叉熵函数可以由最大似然函数在伯努利分布的条件下推导出来，即最小化交叉熵函数的本质就是对数似然函数的最大化。

### 在用sigmoid作为激活函数的时候，为什么要用交叉熵损失函数，而不用均方误差损失函数？

因为sigmoid函数本身的特性，当使用均方误差损失函数时，反向传播的梯度本身会很小，导致参数更新慢；而使用交叉熵损失函数就不会发生这种问题。

详细分析如下：

\>>对于均方误差损失函数，定义为：$$C=\frac{1}{2n} \sum_{i}({a-y})^2$$

其中$y$是我们期望的输出，$a$为神经元的实际输出（$a=\sigma(z), z=wx+b$）。在训练神经网络的时候我们使用梯度下降的方法来更新$w$和$b$，因此需要计算代价函数对$w$和$b$的导数：
$$\frac{\partial C}{\partial w}=(a-y)\sigma^{'}(z)x$$

$$\frac{\partial C}{\partial b}=(a-y)\sigma^{'}(z)$$

然后更新参数$w$和$b$：
$$w=w-\eta\frac{\partial C}{\partial w}=w-\eta(a-y)\sigma^{'}(z)x$$

$$w=w-\eta\frac{\partial C}{\partial b}=w-\eta(a-y)\sigma^{'}(z)$$

因为sigmoid的函数性质，导致$\sigma^{'}(x)$在$z$取大部分值时会很小，从而导致参数$w$和$b$的更新会很慢。

对于交叉熵损失函数，定义为：
$$C=-\frac{1}{n}\sum_{i}[ylna+(1-y)ln(1-a)]$$

其中$y$是我们期望的输出，$a$为神经元的实际输出（$a=\sigma({z}),z=wx+b$）。

。。。。。。待更新




# 参考文献

<div id="loss-function-1"></div>
- [1] [常用损失函数-知乎](https://zhuanlan.zhihu.com/p/58883095)
